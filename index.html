<!DOCTYPE html>
<!-- saved from url=(0023)http://bamos.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" class="gr__bamos_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Chieh Hubert Lin</title>

  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <link rel="alternate" type="application/rss+xml" href="http://hubert0527.github.io/atom.xml">

  <style class="anchorjs"></style><link href="./index_files/bootstrap.min.css" rel="stylesheet">
  <link href="./index_files/font-awesome.min.css" rel="stylesheet">
  <link href="./index_files/academicons.min.css" rel="stylesheet">
  <link href="./index_files/default.css" rel="stylesheet">
  <link href="./index_files/bamos.css" rel="stylesheet">
  <link href="./index_files/sharingbuttons.css" rel="stylesheet">

  <style>
    .paper-teaser-contrainer {
      text-align: center;
    }
    .paper-teaser {
      margin: auto;
    }
  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body data-gr-c-s-loaded="true">
	<div class="navbar navbar-default navbar-fixed-top">
		<div class="container">
			<div class="row">
				<div class="col-md-10 col-md-offset-1">
					<div class="navbar-header">
						  <a href="http://hubert0527.github.io/" class="navbar-brand">
                  <div>
                      <img src="./images/me.jpg" class="img-circle">
                      Chieh Hubert Lin
                  </div>
              </a>
						<button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
						</button>
					</div>
					<div class="navbar-collapse collapse" id="navbar-main">
						<ul class="nav navbar-nav navbar-right" style="font-size: 1.5em">
							<li>
								<a href="http://github.com/hubert0527" target="_blank">
									<i class="fa fa-lg fa-github"></i></a>
							</li>
							<li>
								<a href="https://twitter.com/ChiehHubertLin" target="_blank">
									<i class="fa fa-lg fa-twitter"></i></a>
							</li>
              <li>
                <a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" target="_blank">
                  <i class="ai ai-google-scholar"></i></a>
              </li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>

  <div class="container">
  <div class="row">
    <div class="col-md-5 col-xs-12 col-md-offset-1 vcenter idxHdr">
      <div style="font-size: 2em; color: #4582ec; font-weight: bold; padding-bottom: 0.3em;">Chieh Hubert Lin</div>
      <div style="font-size: 1.2em;">
        Ph.D. student at UC Merced <a href="http://vllab.ucmerced.edu/">Vision and Learning Lab</a>
      </div>
      <br>
      <br>

      <div style="padding: 0.3em; background-color: #4582ec; display: inline-block; border-radius: 4px; font-size: 1.2em;">
        <a href="data/cv.pdf" target="_blank" style="text-decoration: none;">
          <i style="color: white" class="fa fa-download"></i>
        </a>
        <a href="data/cv.pdf" target="_blank" style="color: white; text-decoration: none;"> CV</a>
      </div>

      <br>
      <br>

      <ul class="list-inline idxIcons" style="font-size: 1.9em; margin-top: 0.5em;">
        <li>
          <a href="http://github.com/hubert0527" target="_blank">
            <i class="fa fa-fw fa-github"></i></a>
        </li>
        <li>
          <a href="http://twitter.com/ChiehHubertLin" target="_blank">
            <i class="fa fa-fw fa-twitter"></i></a>
        </li>
        <li>
          <a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" target="_blank">
          <i class="ai ai-google-scholar"></i></a>
        </li>
        <li>
          <a href="http://www.linkedin.com/in/chieh-hubert-lin" target="_blank">
            <i class="fa fa-fw fa-linkedin"></i></a>
        </li>
      </ul>
    </div>
    <div class="col-md-5 col-xs-12 vcenter idxHdr">
      <img src="images/me.jpg" style="border-radius: 20px; margin: 10px; max-width: 300px; width: 100%;" alt="Me.">
      <p style="width: 100%; text-align: center;">Oral presentation in ICCV 2019.</p>
    </div>
  </div>

  <div class="row">
    <div class="col-md-10 col-md-offset-1">
      <hr>

<p>
  I am the first year Ph.D. student supervised by <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>. My research mainly focuses on generative modeling (specifically, GANs) and other random things. Before then, I receive my BS from NTHU in 2018, and worked as a research assistant for two facinating years in <a href="https://aliensunmin.github.io/">Vision and Science Lab</a> in NTHU, supervised by Prof. <a href="https://aliensunmin.github.io/">Min Sun</a> (NTHU), Prof. <a href="https://htchen.github.io/">Hwann-Tzong Chen</a> (NTHU), Prof. <a href="https://walonchiu.github.io/">Wei-Chen Chiu</a> (NCTU), Dr. <a href="https://research.google/people/DaChengJuan/">Da-Cheng Juan</a> (Google), and Dr. <a href="http://www.weiwei.one/">Wei Wei</a> (Google).

  <br><br>

  I graduated from NTHU with an overall GPA 3.09/4.30, and 3.65/4.30 in CS major, which perhaps leads to rejections from top schools without any interview. It sounds like my short-term goal is to show that one can still do something perhaps seems like research without spelling the names of chemical compounds and memorizing the RSA encryption algorithm. 
</p>

<hr>

<h2 id="-education"><i class="fa fa-chevron-right"></i> Education<a class="anchorjs-link " aria-label="Anchor link for:  education" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>

<table class="table table-hover">
  <tbody>
      <tr>
          <td class="col-md-3">Jan 2021 - </td>
          <td>
              <strong>Ph.D. in EECS</strong>
              <br>
              UC Merced
          </td>
        </tr>
    <tr>
      <td class="col-md-3">Sep 2013 - June 2018</td>
      <td>
          <strong>B.S. in Computer Science</strong>
          <br>
        National Tsing Hua University
      </td>
    </tr>
  </tbody>
</table>

<h2 id="-experience"><i class="fa fa-chevron-right"></i> Experience<a class="anchorjs-link " aria-label="Anchor link for:  experience" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody>
<tr>
  <td class="col-md-3">May 2020 - Aug 2020</td>
  <td><strong>Taiwan Military</strong>, Rifleman. <br>
    0108T 206R2B1C</td>
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Oct 2019 - Jan 2020</td>
  <td><strong>Virginia Tech</strong>, Short-term Scholar. <br>
    Supervised by Prof. <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></td>
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Jul 2018 - Sep 2019</td>
  <td><strong>Naitional Tsing Hua University</strong>, Research Assistant. <br>
    Published four papers to top conferences (ICCV, IROS, AAAI), and one paper under submission. <br>
    Supervised by 
    <a href="https://aliensunmin.github.io/">Prof. Min Sun</a>, 
    <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a>, 
    <a href="https://walonchiu.github.io/">Prof. Wei-Chen Chiu</a>, 
    <a href="https://research.google/people/DaChengJuan/">Dr. Da-Cheng Juan</a> and 
    <a href="http://www.weiwei.one/">Dr. Wei Wei</a>.
  </td>
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Sep 2017 - Jun 2018</td>
  <td><strong>National Tsing Hua University</strong>, Undergraduate Research. <br>
    Published one paper to ECCV 2018 as a co-first author. <br>
    Supervised by 
    <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a>, 
    <a href="https://research.google/people/DaChengJuan/">Dr. Da-Cheng Juan</a> and 
    <a href="http://www.weiwei.one/">Dr. Wei Wei</a>.
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Sep 2017 - Jun 2018</td>
  <td><strong>Microsoft Taiwan</strong>, Software Engineering Intern in BingGC team.</td>
</tr>
<tr>
</tr>
</tbody></table>

<h2 id="-selected-publications-"><i class="fa fa-chevron-right"></i> Selected Publications <a class="anchorjs-link " aria-label="Anchor link for:  selected publications " data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>

<p><a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a></p>

<table class="table table-hover">
<tbody>
  
<!-- <tr>
<td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img src="./images/cocogan.png"></td>
<td>
    <strong>Paper Name</strong><br>
    Authors<br>
    Conference<br>
    [<a href="javascript:;" onclick="$(&quot;#abs_something&quot;).toggle()">abs</a>] 
    [<a href="" target="_blank">pdf</a>]  
    [<a href="" target="_blank">website</a>]
    [<a href="" target="_blank">github</a>] <br>
    
    <div id="abs_something" style="text-align: justify; display: none">
      <p>Abs</p>
    </div>
</td>
</tr> -->

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/instanas.gif"></td>
  <td>
      <strong>InstaNAS: Instance-aware Neural Architecture Search</strong><br>
      An-Chieh Cheng*, <strong>Chieh Hubert Lin*</strong>, Da-Cheng Juan, Wei Wei, Min Sun<br>
      AAAI 2020<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_instanas&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1811.10201.pdf" target="_blank">pdf</a>]  
      [<a href="https://hubert0527.github.io/InstaNAS/" target="_blank">website (with demo)</a>]
      [<a href="https://github.com/AnjieZheng/InstaNas" target="_blank">github</a>] <br>
      
      <div id="abs_instanas" style="text-align: justify; display: none">
        <p>Conventional Neural Architecture Search (NAS) aims at finding a single architecture that 
          achieves the best performance, which usually optimizes task related learning objectives such 
          as accuracy. However, a single architecture may not be representative enough for the whole 
          dataset with high diversity and variety. Intuitively, electing domain-expert architectures 
          that are proficient in domain-specific features can further benefit architecture related 
          objectives such as latency. In this paper, we propose InstaNAS---an instance-aware NAS 
          framework---that employs a controller trained to search for a "distribution of architectures" 
          instead of a single architecture; This allows the model to use sophisticated architectures 
          for the difficult samples, which usually comes with large architecture related cost, and 
          shallow architectures for those easy samples. During the inference phase, the controller 
          assigns each of the unseen input samples with a domain expert architecture that can achieve 
          high accuracy with customized inference costs. Experiments within a search space inspired by 
          MobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without compromising 
          accuracy on a series of datasets against MobileNetV2.</p>
      </div>
  </td>
  </tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/cocogan.png"></td>
  <td>
      <strong>COCO-GAN: Generation by Parts via Conditional Coordinating</strong><br>
      <strong>Chieh Hubert Lin</strong>, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen<br>
      ICCV 2019 (<strong style="color:red">oral</strong>)<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_cocogan&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.00284.pdf" target="_blank">pdf (low resolution)</a>]  
      [<a href="https://drive.google.com/drive/folders/1hajegkOkHbnNLcgNaBVMHnwOT2HfSeZn" target="_blank">pdf (high resolution)</a>]  
      [<a href="https://hubert0527.github.io/COCO-GAN/" target="_blank">website</a>]
      [<a href="https://github.com/hubert0527/COCO-GAN" target="_blank">github</a>] <br>
      
      <div id="abs_cocogan" style="text-align: justify; display: none">
        <p>Humans can only interact with part of the surrounding environment due to biological restrictions. 
          Therefore, we learn to reason the spatial relationships across a series of observations to piece 
          together the surrounding environment. Inspired by such behavior and the fact that machines also have 
          computational constraints, we propose <b>CO</b>nditional <b>CO</b>ordinate GAN (COCO-GAN) 
          of which the generator generates images by parts based on their spatial coordinates as the condition. 
          On the other hand, the discriminator learns to justify realism across multiple assembled patches by 
          global coherence, local appearance, and edge-crossing continuity. Despite the full images are never 
          generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} 
          full images during inference. We further demonstrate a variety of novel applications enabled by 
          teaching the network to be aware of coordinates. First, we perform extrapolation to the learned 
          coordinate manifold and generate off-the-boundary patches. Combining with the originally generated 
          full image, COCO-GAN can produce images that are larger than training samples, which we called 
          "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate 
          system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN 
          has a built-in divide-and-conquer paradigm that reduces memory requisition during training and 
          inference, provides high-parallelism, and can generate parts of images on-demand.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6">
    <div style="width: 100%; border: 2px solid #eeeeee">
      <img class="paper-teaser" style="width: 50%; border: none;" src="./images/p2pvg.gif">
    </div>
  </td>
  <td>
      <strong>Point-to-Point Video Generation</strong><br>
      Tsun-Hsuan Wang*, Yen-Chi Cheng*, <strong>Chieh Hubert Lin</strong>, Hwann-Tzong Chen, Min Sun<br>
      ICCV 2019<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_p2pvg&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.02912.pdf" target="_blank">pdf</a>]  
      [<a href="https://zswang666.github.io/P2PVG-Project-Page/" target="_blank">website</a>]
      [<a href="https://github.com/yccyenchicheng/p2pvg" target="_blank">github</a>] <br>
      
      <div id="abs_p2pvg" style="text-align: justify; display: none">
        <p>While image manipulation achieves tremendous breakthroughs (e.g., generating realistic faces) in 
          recent years, video generation is much less explored and harder to control, which limits its applications 
          in the real world. For instance, video editing requires temporal coherence across multiple clips and 
          thus poses both start and end constraints within a video sequence. We introduce point-to-point video 
          generation that controls the generation process with two control points: the targeted start- and end-frames. 
          The task is challenging since the model not only generates a smooth transition of frames, but also 
          plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of 
          various length. We propose to maximize the modified variational lower bound of conditional data 
          likelihood under a skip-frame training strategy. Our model can generate sequences such that their 
          end-frame is consistent with the targeted end-frame without loss of quality and diversity. Extensive 
          experiments are conducted on Stochastic Moving MNIST, Weizmann Human Action, and Human3.6M to evaluate 
          the effectiveness of the proposed method. We demonstrate our method under a series of scenarios 
          (e.g., dynamic length generation) and the qualitative results showcase the potential and merits of 
          point-to-point generation.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/stereo_lidar_ccvnorm.gif"></td>
  <td>
      <strong>3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization</strong><br>
      Tsun-Hsuan Wang, Hou-Ning Hu, <strong>Chieh Hubert Lin</strong>, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun<br>
      IROS 2019<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_ccvnorm&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.02917.pdf" target="_blank">pdf</a>]  
      [<a href="https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/" target="_blank">website</a>]
      [<a href="https://github.com/zswang666/Stereo-LiDAR-CCVNorm" target="_blank">github</a>] <br>
      
      <div id="abs_ccvnorm" style="text-align: justify; display: none">
        <p>The complementary characteristics of active and passive depth sensing techniques motivate the 
          fusion of the Li-DAR sensor and stereo camera for improved depth perception. Instead of directly 
          fusing estimated depths across LiDAR and stereo modalities, we take advantages of the stereo 
          matching network with two enhanced techniques: Input Fusion and Conditional Cost Volume 
          Normalization (CCVNorm) on the LiDAR information. The proposed framework is generic and closely 
          integrated with the cost volume component that is commonly utilized in stereo matching neural networks. 
          We experimentally verify the efficacy and robustness of our method on the KITTI Stereo and Depth 
          Completion datasets, obtaining favorable performance against various fusion strategies. Moreover, 
          we demonstrate that, with a hierarchical extension of CCVNorm, the proposed method brings only slight 
          overhead to the stereo matching network in terms of computation time and model size.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/began-cs.png"></td>
  <td>
      <strong>Escaping from Collapsing Modes in a Constrained Space</strong><br>
      Chia-Che Chang*, <strong>Chieh Hubert Lin*</strong>, Che-Rung Lee, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen<br>
      ECCV 2018<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_begancs&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1808.07258.pdf" target="_blank">pdf</a>] 
      [<a href="https://github.com/chang810249/BEGAN-CS" target="_blank">github</a>] <br>
      
      <div id="abs_begancs" style="text-align: justify; display: none">
        <p>Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. 
          We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), 
          which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, 
          we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, 
          called <b>BEGAN with a Constrained Space</b> (BEGAN-CS), which includes a latent-space constraint in the 
          loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse 
          without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution 
          of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has 
          additional advantages of being able to train on small datasets and to generate images similar to a given real 
          image yet with variations of designated attributes on-the-fly.</p>
      </div>
  </td>
</tr>

</tbody></table>


<h2 id="-honors--awards"><i class="fa fa-chevron-right"></i> Honors &amp; Awards<a class="anchorjs-link " aria-label="Anchor link for:  honors  awards" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody><tr>
  <td class="col-md-2">2018</td>
  <td>
      One time Yahoo AI Scholarship
    <!--  -->
  </td>
</tr>
<tr>
  <td class="col-md-2">2018 - 2019</td>
  <td>
    Two times Appier AI Scholarship
    <!--  -->
  </td>
</tr>
</tbody></table>


<h2 id="-service"><i class="fa fa-chevron-right"></i> Service<a class="anchorjs-link " aria-label="Anchor link for:  service" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody><tr>
  <td class="col-md-2">Reviewer</td>
  <td>
      <p>
        AAAI 2019, 
        ICCV 2019,
        AAAI 2020,
        ICML 2020,
        CVPR 2020
      </p>
    </td>
</tr>
</tbody></table>

<hr>

<div style="width: 100%;">
  <div style="display: inline;"> Last updated on 2020-06-25. </div>
  <div style="float: right; display: inline;">Forked from <a href="http://bamos.github.io/">Brandon Amos</a></div> 
</div>

    </div>
  </div>
</div>


  <script async="" src="./index_files/analytics.js"></script><script src="./index_files/sp.js"></script>
  <script src="./index_files/jquery.min.js"></script>
  <script src="./index_files/bootstrap.min.js"></script>
  <script src="./index_files/anchor.min.js"></script>
  <script src="./index_files/jquery.toc.js"></script>
  <script type="text/javascript">
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
   })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

   ga('create', 'UA-82988202-2', 'auto');
   ga('send', 'pageview');

   $("#toc").toc({
       'headings': 'h2,h3'
   });
   anchors.add('h2,h3');
  </script>




</body></html>
<!DOCTYPE html>
<!-- saved from url=(0023)http://bamos.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" class="gr__bamos_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Chieh Hubert Lin (林杰)</title>
  <meta name="keywords" content="Chieh Hubert Lin,Chieh Lin,Hubert Lin,Hubert,林杰,OuO">

  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <link rel="alternate" type="application/rss+xml" href="http://hubert0527.github.io/atom.xml">

  <style class="anchorjs"></style><link href="./index_files/bootstrap.min.css" rel="stylesheet">
  <link href="./index_files/font-awesome.min.css" rel="stylesheet">
  <link href="./index_files/academicons.min.css" rel="stylesheet">
  <link href="./index_files/default.css" rel="stylesheet">
  <link href="./index_files/bamos.css" rel="stylesheet">
  <link href="./index_files/sharingbuttons.css" rel="stylesheet">

  <link rel="shortcut icon" href="images/favicon.ico"/>
  <link rel="bookmark" href="images/favicon.ico"/>

  <style>
    .paper-teaser-contrainer {
      text-align: center;
    }
    .paper-teaser {
      margin: auto;
    }

    h2 {
      padding-top: 3cm;
    }
  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body data-gr-c-s-loaded="true">
	<div class="navbar navbar-default navbar-fixed-top">
		<div class="container">
			<div class="row">
				<div class="col-md-10 col-md-offset-1">
					<div class="navbar-header">
						  <a href="http://hubert0527.github.io/" class="navbar-brand">
                  <div>
                      <img src="./images/me.jpg" class="img-circle">
                      Chieh Hubert Lin
                  </div>
              </a>
						<button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
						</button>
					</div>
					<div class="navbar-collapse collapse" id="navbar-main">
						<ul class="nav navbar-nav navbar-right" style="font-size: 1.5em">
							<li>
								<a href="http://github.com/hubert0527" target="_blank">
									<i class="fa fa-lg fa-github"></i></a>
							</li>
							<li>
								<a href="https://twitter.com/ChiehHubertLin" target="_blank">
									<i class="fa fa-lg fa-twitter"></i></a>
							</li>
              <li>
                <a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" target="_blank">
                  <i class="ai ai-google-scholar"></i></a>
              </li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>

  <div class="container">
  <div class="row">
    <div class="col-md-5 col-xs-12 col-md-offset-1 vcenter idxHdr">
      <div style="font-size: 2em; color: #4582ec; font-weight: bold; padding-bottom: 0.3em;">Chieh Hubert Lin</div>
      <div style="font-size: 1.2em;">
        Ph.D. student at UC Merced <a href="http://vllab.ucmerced.edu/">Vision and Learning Lab</a>
      </div>
      <br>
      <br>

      <div style="padding: 0.3em; background-color: #4582ec; display: inline-block; border-radius: 4px; font-size: 1.2em;">
        <a href="data/cv.pdf" target="_blank" style="text-decoration: none;">
          <i style="color: white" class="fa fa-download"></i>
        </a>
        <a href="data/cv.pdf" target="_blank" style="color: white; text-decoration: none;"> CV</a>
      </div>

      <br>
      <br>

      <ul class="list-inline idxIcons" style="font-size: 1.9em; margin-top: 0.5em;">
        <li>
          <a href="http://github.com/hubert0527" target="_blank">
            <i class="fa fa-fw fa-github"></i></a>
        </li>
        <li>
          <a href="http://twitter.com/ChiehHubertLin" target="_blank">
            <i class="fa fa-fw fa-twitter"></i></a>
        </li>
        <li>
          <a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" target="_blank">
          <i class="ai ai-google-scholar"></i></a>
        </li>
        <li>
          <a href="http://www.linkedin.com/in/chieh-hubert-lin" target="_blank">
            <i class="fa fa-fw fa-linkedin"></i></a>
        </li>
      </ul>
    </div>
    <div class="col-md-5 col-xs-12 vcenter idxHdr">
      <img src="images/me.jpg" style="border-radius: 20px; margin: 10px; max-width: 300px; width: 100%;" alt="Me.">
    </div>
  </div>

  <div class="row">
    <div class="col-md-10 col-md-offset-1">
      <hr>

<p>
  I am a third-year Ph.D. student at UC Merced supervised by <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>.
  My research mainly focuses on generative modeling in unconstrained scenes, and some other random things.
  In the past few years, I fortunately have the chance to collaborate with <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a>, <a href="http://www.stulyakov.com/">Sergey Tulyakov</a> and <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>.
  And many facinating people, but the list is too long, please forgive me QuQ.
</p>

<hr>

<h2 id="-education"><i class="fa fa-chevron-right"></i> Education<a class="anchorjs-link " aria-label="Anchor link for:  education" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>

<table class="table table-hover">
  <tbody>
      <tr>
          <td class="col-md-3">Jan 2021 - </td>
          <td>
              <strong>Ph.D. in EECS</strong>
              <br>
              UC Merced
          </td>
        </tr>
    <tr>
      <td class="col-md-3">Sep 2013 - June 2018</td>
      <td>
          <strong>B.S. in Computer Science</strong>
          <br>
        National Tsing Hua University
      </td>
    </tr>
  </tbody>
</table>

<h2 id="-experience"><i class="fa fa-chevron-right"></i> Experience<a class="anchorjs-link " aria-label="Anchor link for:  experience" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody>
  <tr>
    <td class="col-md-3">May 2022 - </td>
    <td><strong>Snap Inc.</strong> Research Intern.</td>
    Working with 
    <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a>, 
    <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>,
    <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">Aliaksandr Siarohin</a>, 
    <a href="https://mlchai.com/">Menglei Chai</a> and
    <a href="https://www.willimenapace.com/">Willi Menapace</a>.
  </tr>
  <tr>
  </tr>
<tr>
  <td class="col-md-3">May 2020 - Aug 2020</td>
  <td><strong>Military Service in Taiwan</strong>.</td>
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Jul 2018 - Sep 2019</td>
  <td><strong>Naitional Tsing Hua University</strong>, Research Assistant. <br>
    Published four papers to top conferences (ICCV, IROS, AAAI), and one paper under submission. <br>
    Working with
    <a href="https://aliensunmin.github.io/">Prof. Min Sun</a>, 
    <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a>, 
    <a href="https://walonchiu.github.io/">Prof. Wei-Chen Chiu</a>, 
    <a href="https://research.google/people/DaChengJuan/">Dr. Da-Cheng Juan</a> and 
    <a href="http://www.weiwei.one/">Dr. Wei Wei</a>.
  </td>
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Sep 2017 - Jun 2018</td>
  <td><strong>National Tsing Hua University</strong>, Undergraduate Research. <br>
    Published one paper to ECCV 2018 as a co-first author. <br>
    Working with 
    <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a>, 
    <a href="https://research.google/people/DaChengJuan/">Dr. Da-Cheng Juan</a> and 
    <a href="http://www.weiwei.one/">Dr. Wei Wei</a>.
</tr>
<tr>
</tr>
<tr>
  <td class="col-md-3">Sep 2017 - Jun 2018</td>
  <td><strong>Microsoft Taiwan</strong>, Software Engineering Intern in BingGC team.</td>
</tr>
<tr>
</tr>
</tbody></table>

<h2 id="-selected-publications-"><i class="fa fa-chevron-right"></i> Publications <a class="anchorjs-link " aria-label="Anchor link for:  selected publications " data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>

<p><a href="https://scholar.google.com/citations?user=MJcnTa4AAAAJ" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a></p>

<table class="table table-hover">
<tbody>
  
<!-- <tr>
<td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img src="./images/cocogan.png"></td>
<td>
    <strong>Paper Name</strong><br>
    Authors<br>
    Conference<br>
    [<a href="javascript:;" onclick="$(&quot;#abs_something&quot;).toggle()">abs</a>] 
    [<a href="" target="_blank">pdf</a>]  
    [<a href="" target="_blank">website</a>]
    [<a href="" target="_blank">github</a>] <br>
    
    <div id="abs_something" style="text-align: justify; display: none">
      <p>Abs</p>
    </div>
</td>
</tr> -->




<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/infiniCity.png"></td>
  <td>
      <strong>InfiniCity: Infinite-Scale City Synthesis</strong><br>
      <strong>Chieh Hubert Lin</strong>, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Sergey Tulyakov, Ming-Hsuan Yang <br>
      Under Submission <br>
      [<a href="javascript:;" onclick="$(&quot;#abs_infinicity&quot;).toggle()">abs</a>]
      <div id="abs_infinicity" style="text-align: justify; display: none">
        <p>We propose InfiniCity, a three-stage synthesis framework toward infinite-scale city scene synthesis. (More details coming soon...) </p>
      </div>
  </td>
</tr>

<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/PPP.png"></td>
  <td>
      <strong>Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features</strong><br>
      <strong>Chieh Hubert Lin, Hsin-Ying Lee, Hung-Yu Tseng, Maneesh Singh, Ming-Hsuan Yang<br>
      Under Submission <br>
      [<a href="javascript:;" onclick="$(&quot;#abs_ppp&quot;).toggle()">abs</a>]
      [<a href="https://arxiv.org/abs/2206.01202b" target="_blank">paper</a>]
      <div id="abs_ppp" style="text-align: justify; display: none">
        <p>Recent studies have shown that paddings in convolutional neural networks encode
          absolute position information which can negatively affect the model performance
          for certain tasks. However, existing metrics for quantifying the strength of 
          positional information remain unreliable and frequently lead to erroneous results.
          To address this issue, we propose novel metrics for measuring and visualizing
          the encoded positional information. We formally define the encoded information
          as Position-information Pattern from Padding (PPP) and conduct a series of 
          experiments to study its properties as well as its formation. The proposed metrics
          measure the presence of positional information more reliably than the existing
          metrics based on PosENet and tests in F-Conv. We also demonstrate that for any
          extant (and proposed) padding schemes, PPP is primarily a learning artifact and is
          less dependent on the characteristics of the underlying padding schemes.</p>
      </div>
  </td>
</tr>

<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/infinityGAN.png"></td>
  <td>
      <strong>InfinityGAN: Towards Infinite-Pixel Image Synthesis</strong><br>
      <strong>Chieh Hubert Lin</strong>, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang <br>
      ICLR 2022 <br>
      [<a href="javascript:;" onclick="$(&quot;#abs_infgan&quot;).toggle()">abs</a>] 
      [<a href="https://openreview.net/forum?id=ufGMqIM0a4b" target="_blank">paper</a>] 
      [<a href="https://hubert0527.github.io/infinityGAN/" target="_blank">project page</a>]
      [<a href="https://github.com/hubert0527/infinityGAN" target="_blank">codes (PyTorch)</a>] <br>
      
      <div id="abs_infgan" style="text-align: justify; display: none">
        <p>We present InfinityGAN, a method to generate arbitrary-sized images. The problem is associated with several key 
        challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, both in terms 
        of computation and availability of large-field-of-view training data. InfinityGAN trains and infers patch-by-patch 
        seamlessly with low computational resources. Second, large images should be locally and globally consistent, avoid 
        repetitive patterns, and look realistic. To address these, InfinityGAN takes global appearance, local structure and 
        texture into account. With this formulation, we can generate images with spatial size and level of detail not attainable 
        before. Experimental evaluation supports that InfinityGAN generates images with superior global structure compared to 
        baselines and features parallelizable inference. Finally, we show several applications unlocked by our approach, such as 
        fusing styles spatially, multi-modal outpainting and image inbetweening at arbitrary input and output sizes.</p>
      </div>
  </td>
</tr>

<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/inout.png"></td>
  <td>
      <strong>In&Out : Diverse Image Outpainting via GAN Inversion</strong><br>
      Yen-Chi Cheng, <strong>Chieh Hubert Lin</strong>, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang <br>
      CVPR 2022 <br>
      [<a href="javascript:;" onclick="$(&quot;#abs_inout&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/abs/2104.00675" target="_blank">paper</a>]  
      [<a href="https://yccyenchicheng.github.io/InOut/" target="_blank">project page</a>]
      [<a href="https://github.com/yccyenchicheng/InOut" target="_blank">codes (TBA)</a>] <br>
      
      <div id="abs_inout" style="text-align: justify; display: none">
        <p>Image outpainting seeks for a semantically consistent extension of the input image beyond its available content. 
          Compared to inpainting -- filling in missing pixels in a way coherent with the neighboring pixels -- outpainting 
          can be achieved in more diverse ways since the problem is less constrained by the surrounding pixels. Existing 
          image outpainting methods pose the problem as a conditional image-to-image translation task, often generating 
          repetitive structures and textures by replicating the content available in the input image. In this work, we formulate 
          the problem from the perspective of inverting generative adversarial networks. Our generator renders micro-patches 
          conditioned on their joint latent code as well as their individual positions in the image. To outpaint an image, we 
          seek for multiple latent codes not only recovering available patches but also synthesizing diverse outpainting by 
          patch-based generation. This leads to richer structure and content in the outpainted regions. Furthermore, our 
          formulation allows for outpainting conditioned on the categorical input, thereby enabling flexible user controls. 
          Extensive experimental results demonstrate the proposed method performs favorably against existing in- and outpainting 
          methods, featuring higher visual quality and diversity.</p>
      </div>
  </td>
</tr>


<tr>
  <td><h3 style="margin: 0; padding: 0">Pre-Ph.D.</h3></td>
  <!-- <td></td> -->
</tr>

<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/instanas.gif"></td>
  <td>
      <strong>InstaNAS: Instance-aware Neural Architecture Search</strong><br>
      An-Chieh Cheng*, <strong>Chieh Hubert Lin*</strong>, Da-Cheng Juan, Wei Wei, Min Sun<br>
      AAAI 2020<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_instanas&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1811.10201.pdf" target="_blank">paper</a>]  
      [<a href="https://hubert0527.github.io/InstaNAS/" target="_blank">project page (w/ demo)</a>]
      [<a href="https://github.com/AnjieZheng/InstaNas" target="_blank">codes</a>] <br>
      
      <div id="abs_instanas" style="text-align: justify; display: none">
        <p>Conventional Neural Architecture Search (NAS) aims at finding a single architecture that 
          achieves the best performance, which usually optimizes task related learning objectives such 
          as accuracy. However, a single architecture may not be representative enough for the whole 
          dataset with high diversity and variety. Intuitively, electing domain-expert architectures 
          that are proficient in domain-specific features can further benefit architecture related 
          objectives such as latency. In this paper, we propose InstaNAS---an instance-aware NAS 
          framework---that employs a controller trained to search for a "distribution of architectures" 
          instead of a single architecture; This allows the model to use sophisticated architectures 
          for the difficult samples, which usually comes with large architecture related cost, and 
          shallow architectures for those easy samples. During the inference phase, the controller 
          assigns each of the unseen input samples with a domain expert architecture that can achieve 
          high accuracy with customized inference costs. Experiments within a search space inspired by 
          MobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without compromising 
          accuracy on a series of datasets against MobileNetV2.</p>
      </div>
  </td>
  </tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/cocogan.png"></td>
  <td>
      <strong>COCO-GAN: Generation by Parts via Conditional Coordinating</strong><br>
      <strong>Chieh Hubert Lin</strong>, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen<br>
      ICCV 2019 (<strong style="color:red">oral</strong>)<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_cocogan&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.00284.pdf" target="_blank">paper (low resolution)</a>]  
      [<a href="https://drive.google.com/drive/folders/1hajegkOkHbnNLcgNaBVMHnwOT2HfSeZn" target="_blank">paper (high resolution)</a>]  
      [<a href="https://hubert0527.github.io/COCO-GAN/" target="_blank">project page</a>]
      [<a href="https://github.com/hubert0527/COCO-GAN" target="_blank">codes</a>] <br>
      
      <div id="abs_cocogan" style="text-align: justify; display: none">
        <p>Humans can only interact with part of the surrounding environment due to biological restrictions. 
          Therefore, we learn to reason the spatial relationships across a series of observations to piece 
          together the surrounding environment. Inspired by such behavior and the fact that machines also have 
          computational constraints, we propose <b>CO</b>nditional <b>CO</b>ordinate GAN (COCO-GAN) 
          of which the generator generates images by parts based on their spatial coordinates as the condition. 
          On the other hand, the discriminator learns to justify realism across multiple assembled patches by 
          global coherence, local appearance, and edge-crossing continuity. Despite the full images are never 
          generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} 
          full images during inference. We further demonstrate a variety of novel applications enabled by 
          teaching the network to be aware of coordinates. First, we perform extrapolation to the learned 
          coordinate manifold and generate off-the-boundary patches. Combining with the originally generated 
          full image, COCO-GAN can produce images that are larger than training samples, which we called 
          "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate 
          system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN 
          has a built-in divide-and-conquer paradigm that reduces memory requisition during training and 
          inference, provides high-parallelism, and can generate parts of images on-demand.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6">
    <div style="width: 100%; border: 2px solid #eeeeee">
      <img class="paper-teaser" style="width: 50%; border: none;" src="./images/p2pvg.gif">
    </div>
  </td>
  <td>
      <strong>Point-to-Point Video Generation</strong><br>
      Tsun-Hsuan Wang*, Yen-Chi Cheng*, <strong>Chieh Hubert Lin</strong>, Hwann-Tzong Chen, Min Sun<br>
      ICCV 2019<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_p2pvg&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.02912.pdf" target="_blank">paper</a>]  
      [<a href="https://zswang666.github.io/P2PVG-Project-Page/" target="_blank">project page</a>]
      [<a href="https://github.com/yccyenchicheng/p2pvg" target="_blank">codes</a>] <br>
      
      <div id="abs_p2pvg" style="text-align: justify; display: none">
        <p>While image manipulation achieves tremendous breakthroughs (e.g., generating realistic faces) in 
          recent years, video generation is much less explored and harder to control, which limits its applications 
          in the real world. For instance, video editing requires temporal coherence across multiple clips and 
          thus poses both start and end constraints within a video sequence. We introduce point-to-point video 
          generation that controls the generation process with two control points: the targeted start- and end-frames. 
          The task is challenging since the model not only generates a smooth transition of frames, but also 
          plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of 
          various length. We propose to maximize the modified variational lower bound of conditional data 
          likelihood under a skip-frame training strategy. Our model can generate sequences such that their 
          end-frame is consistent with the targeted end-frame without loss of quality and diversity. Extensive 
          experiments are conducted on Stochastic Moving MNIST, Weizmann Human Action, and Human3.6M to evaluate 
          the effectiveness of the proposed method. We demonstrate our method under a series of scenarios 
          (e.g., dynamic length generation) and the qualitative results showcase the potential and merits of 
          point-to-point generation.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/stereo_lidar_ccvnorm.gif"></td>
  <td>
      <strong>3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization</strong><br>
      Tsun-Hsuan Wang, Hou-Ning Hu, <strong>Chieh Hubert Lin</strong>, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun<br>
      IROS 2019<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_ccvnorm&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1904.02917.pdf" target="_blank">paper</a>]  
      [<a href="https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/" target="_blank">project page</a>]
      [<a href="https://github.com/zswang666/Stereo-LiDAR-CCVNorm" target="_blank">codes</a>] <br>
      
      <div id="abs_ccvnorm" style="text-align: justify; display: none">
        <p>The complementary characteristics of active and passive depth sensing techniques motivate the 
          fusion of the Li-DAR sensor and stereo camera for improved depth perception. Instead of directly 
          fusing estimated depths across LiDAR and stereo modalities, we take advantages of the stereo 
          matching network with two enhanced techniques: Input Fusion and Conditional Cost Volume 
          Normalization (CCVNorm) on the LiDAR information. The proposed framework is generic and closely 
          integrated with the cost volume component that is commonly utilized in stereo matching neural networks. 
          We experimentally verify the efficacy and robustness of our method on the KITTI Stereo and Depth 
          Completion datasets, obtaining favorable performance against various fusion strategies. Moreover, 
          we demonstrate that, with a hierarchical extension of CCVNorm, the proposed method brings only slight 
          overhead to the stereo matching network in terms of computation time and model size.</p>
      </div>
  </td>
</tr>

  
<tr>
  <td class="paper-teaser-contrainer col-lg-4 col-md-6 col-xs-6"><img class="paper-teaser" src="./images/began-cs.png"></td>
  <td>
      <strong>Escaping from Collapsing Modes in a Constrained Space</strong><br>
      Chia-Che Chang*, <strong>Chieh Hubert Lin*</strong>, Che-Rung Lee, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen<br>
      ECCV 2018<br>
      [<a href="javascript:;" onclick="$(&quot;#abs_begancs&quot;).toggle()">abs</a>] 
      [<a href="https://arxiv.org/pdf/1808.07258.pdf" target="_blank">paper</a>] 
      [<a href="https://github.com/chang810249/BEGAN-CS" target="_blank">codes</a>] <br>
      
      <div id="abs_begancs" style="text-align: justify; display: none">
        <p>Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. 
          We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), 
          which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, 
          we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, 
          called <b>BEGAN with a Constrained Space</b> (BEGAN-CS), which includes a latent-space constraint in the 
          loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse 
          without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution 
          of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has 
          additional advantages of being able to train on small datasets and to generate images similar to a given real 
          image yet with variations of designated attributes on-the-fly.</p>
      </div>
  </td>
</tr>

</tbody></table>


<h2 id="-honors--awards"><i class="fa fa-chevron-right"></i> Honors &amp; Awards<a class="anchorjs-link " aria-label="Anchor link for:  honors  awards" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody>
<tr>
  <td class="col-md-2">2021 - 2022</td>
  <td>
    Three times Bobcat Fellowship
    <!--  -->
  </td>
</tr>
<tr>
  <td class="col-md-2">2018 - 2019</td>
  <td>
    Two times Appier AI Scholarship
    <!--  -->
  </td>
</tr>
<tr>
  <td class="col-md-2">2018</td>
  <td>
      Yahoo AI Scholarship
    <!--  -->
  </td>
</tr>
</tbody></table>


<!-- <h2 id="-service"><i class="fa fa-chevron-right"></i> Service<a class="anchorjs-link " aria-label="Anchor link for:  service" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>
<table class="table table-hover">
<tbody><tr>
  <td class="col-md-2">Reviewer</td>
  <td>
      <p>
        AAAI 2019, 
        ICCV 2019,
        AAAI 2020,
        ICML 2020,
        CVPR 2020
      </p>
    </td>
</tr>
</tbody></table> -->

<!-- <h2 id="-misc"><i class="fa fa-chevron-right"></i> Misc<a class="anchorjs-link " aria-label="Anchor link for:  misc" data-anchorjs-icon="" style="font-family: anchorjs-icons; font-style: normal; font-variant: normal; font-weight: normal; line-height: 1; padding-left: 0.375em;"></a></h2>

<table class="table table-hover"><tbody>
  
<tr>
  <td class="col-md-2"><a href="https://en.wikipedia.org/wiki/Go_(game)">Go</a></td>
  <td>I'm an amateur 2 dan player.</td>
  <td></td>
</tr>

<tr>
  <td class="col-md-2"><a href="https://www.aimer-web.jp/">Aimer</a></td>
  <td>
    I'm fan #225887 of Aimer's fan club!!
  </td>
</tr>

<tr>
  <td class="col-md-2"><a href="https://reol.jp/">Reol</a></td>
  </td>
</tr> -->

<!-- <tr>
  <td class="col-md-2">OuO enthusiast</td>
  <td>
    OuO expresses my happiness. <br>
    OuO expresses my panic. <br>
    OuO expresses my anger. <br>
    ouO expresses my confusion. <br>
    OuO is the answer to the universe! <br>
    You should start replacing all your emojis with OuO from now on OuO!
  </td>
  <td>OUO</td>
</tr> -->

</tbody></table>


<hr>

<div style="width: 100%;">
  <div style="display: inline;"> Last updated on 2022-11-14. </div>
  <div style="float: right; display: inline;">Forked from <a href="http://bamos.github.io/">Brandon Amos</a></div> 
</div>

    </div>
  </div>
</div>


  <script async="" src="./index_files/analytics.js"></script><script src="./index_files/sp.js"></script>
  <script src="./index_files/jquery.min.js"></script>
  <script src="./index_files/bootstrap.min.js"></script>
  <script src="./index_files/anchor.min.js"></script>
  <script src="./index_files/jquery.toc.js"></script>
  <script type="text/javascript">
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
   })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

   ga('create', 'UA-82988202-2', 'auto');
   ga('send', 'pageview');

   $("#toc").toc({
       'headings': 'h2,h3'
   });
   anchors.add('h2,h3');
  </script>




</body></html>